"""
GPT APIìš© ì˜ìƒ ë°ì´í„° ì¶”ì¶œ íŒŒì´í”„ë¼ì¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì˜ìƒì—ì„œ ë‹¤ìŒì„ ì¶”ì¶œí•©ë‹ˆë‹¤:
1. ëŒ€ì‚¬ (Whisper STT)
2. ë¬´ìŒ êµ¬ê°„ (Silero VAD)
3. 2fps í”„ë ˆì„ ì´ë¯¸ì§€

ì¶œë ¥:
- JSON íŒŒì¼: ëŒ€ì‚¬ ë° ë¬´ìŒ êµ¬ê°„ ì •ë³´ (ì‹œê°„ ì •ë³´ í¬í•¨)
- ì´ë¯¸ì§€ í´ë”: 2fpsë¡œ ì¶”ì¶œëœ í”„ë ˆì„ ì´ë¯¸ì§€ (íŒŒì¼ëª…ì— ì‹œê°„ ì •ë³´ í¬í•¨)
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass, asdict

import torch
import torchaudio
import ffmpeg
import scipy.io.wavfile as wavfile
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class SpeechSegment:
    """ëŒ€ì‚¬ êµ¬ê°„ ì •ë³´"""
    id: int
    start_time: float  # ì´ˆ ë‹¨ìœ„
    end_time: float
    duration: float
    text: str
    type: str = "speech"

@dataclass  
class SilenceSegment:
    """ë¬´ìŒ êµ¬ê°„ ì •ë³´"""
    id: int
    start_time: float  # ì´ˆ ë‹¨ìœ„
    end_time: float
    duration: float
    type: str = "silence"

@dataclass
class FrameInfo:
    """í”„ë ˆì„ ì´ë¯¸ì§€ ì •ë³´"""
    frame_number: int
    timestamp: float  # ì´ˆ ë‹¨ìœ„
    filename: str


def extract_audio(input_video: str, output_audio: str, sample_rate: int = 16000) -> str:
    """
    ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ëª¨ë…¸, ì§€ì •ëœ ìƒ˜í”Œë ˆì´íŠ¸).
    
    Args:
        input_video: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        output_audio: ì¶œë ¥ ì˜¤ë””ì˜¤ ê²½ë¡œ
        sample_rate: ìƒ˜í”Œë ˆì´íŠ¸ (ê¸°ë³¸: 16000)
    
    Returns:
        ì¶œë ¥ ì˜¤ë””ì˜¤ ê²½ë¡œ
    """
    logger.info(f"[Audio] ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘: {input_video} -> {output_audio}")
    
    (
        ffmpeg
        .input(input_video)
        .output(output_audio, ac=1, ar=sample_rate)
        .overwrite_output()
        .run(quiet=True)
    )
    
    logger.info(f"[Audio] ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ: {output_audio}")
    return output_audio


def load_vad_model():
    """Silero VAD ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    logger.info("[VAD] Silero VAD ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    import sys
    import os
    
    # stdout/stderrë¥¼ ì™„ì „íˆ ì–µì œ (torch.hub ë‹¤ìš´ë¡œë“œ ë©”ì‹œì§€ ì°¨ë‹¨)
    with open(os.devnull, 'w') as devnull:
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        try:
            sys.stdout = devnull
            sys.stderr = devnull
            model, utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                trust_repo=True,
                verbose=False,
            )
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr
    
    get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks = utils
    logger.info("[VAD] ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    return model, get_speech_timestamps


def detect_speech_segments(
    audio_path: str,
    vad_model,
    get_speech_timestamps_fn,
    min_silence_duration: float = 2.5
) -> Tuple[List[Dict], List[Dict], int]:
    """
    VADë¥¼ ì‚¬ìš©í•˜ì—¬ ë°œí™”/ë¬´ìŒ êµ¬ê°„ì„ íƒì§€í•©ë‹ˆë‹¤.
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        vad_model: Silero VAD ëª¨ë¸
        get_speech_timestamps_fn: ë°œí™” êµ¬ê°„ íƒì§€ í•¨ìˆ˜
        min_silence_duration: ë¬´ìŒìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ì‹œê°„ (ì´ˆ)
    
    Returns:
        (ë°œí™” êµ¬ê°„ ë¦¬ìŠ¤íŠ¸, ë¬´ìŒ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸, ìƒ˜í”Œë ˆì´íŠ¸)
    """
    logger.info(f"[VAD] ë°œí™”/ë¬´ìŒ êµ¬ê°„ íƒì§€ ì¤‘: {audio_path}")
    
    # scipyë¥¼ ì‚¬ìš©í•˜ì—¬ wav íŒŒì¼ ë¡œë“œ (torchaudio í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
    sr, wav_np = wavfile.read(audio_path)
    
    # numpy dtypeì„ í™•ì¸í•˜ê³  ì •ê·œí™” (ë³€í™˜ ì „ì— í™•ì¸í•´ì•¼ í•¨)
    if wav_np.dtype == np.int16:
        wav = torch.from_numpy(wav_np.astype(np.float32)) / 32768.0
    elif wav_np.dtype == np.int32:
        wav = torch.from_numpy(wav_np.astype(np.float32)) / 2147483648.0
    elif wav_np.dtype == np.float32:
        wav = torch.from_numpy(wav_np)
    else:
        wav = torch.from_numpy(wav_np.astype(np.float32))
    
    # ëª¨ë…¸ ì±„ë„ì´ë©´ ì°¨ì› ì¶”ê°€
    if wav.dim() == 1:
        wav = wav.unsqueeze(0)
    
    speech_timestamps = get_speech_timestamps_fn(wav[0], vad_model, sampling_rate=sr)
    
    speech_segments = []
    silence_segments = []
    
    prev_end = 0.0
    speech_id = 1
    silence_id = 1
    
    for seg in speech_timestamps:
        start = seg["start"] / sr
        end = seg["end"] / sr
        
        # ë¬´ìŒ êµ¬ê°„ (ì´ì „ ë°œí™” ë ~ í˜„ì¬ ë°œí™” ì‹œì‘)
        if start - prev_end >= min_silence_duration:
            silence_segments.append({
                "id": silence_id,
                "start_time": round(prev_end, 3),
                "end_time": round(start, 3),
                "duration": round(start - prev_end, 3),
                "type": "silence"
            })
            silence_id += 1
        
        # ë°œí™” êµ¬ê°„
        speech_segments.append({
            "id": speech_id,
            "start_time": round(start, 3),
            "end_time": round(end, 3),
            "duration": round(end - start, 3),
            "start_sample": seg["start"],
            "end_sample": seg["end"],
            "type": "speech"
        })
        speech_id += 1
        prev_end = end
    
    # ë§ˆì§€ë§‰ ë°œí™” ì´í›„ì˜ ë¬´ìŒ êµ¬ê°„ (ì˜¤ë””ì˜¤ ëê¹Œì§€)
    audio_duration = wav.shape[1] / sr
    if audio_duration - prev_end >= min_silence_duration:
        silence_segments.append({
            "id": silence_id,
            "start_time": round(prev_end, 3),
            "end_time": round(audio_duration, 3),
            "duration": round(audio_duration - prev_end, 3),
            "type": "silence"
        })
    
    logger.info(f"[VAD] ë°œí™” êµ¬ê°„: {len(speech_segments)}ê°œ, ë¬´ìŒ êµ¬ê°„: {len(silence_segments)}ê°œ")
    return speech_segments, silence_segments, sr


def transcribe_speech_segments(
    audio_path: str,
    speech_segments: List[Dict],
    sample_rate: int,
    language: str = "en",
    whisper_model_size: str = "medium",
    temp_dir: str = None
) -> List[Dict]:
    """
    Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ë°œí™” êµ¬ê°„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•©ë‹ˆë‹¤.
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        speech_segments: ë°œí™” êµ¬ê°„ ë¦¬ìŠ¤íŠ¸
        sample_rate: ìƒ˜í”Œë ˆì´íŠ¸
        language: ì–¸ì–´ ì½”ë“œ (en, ko ë“±)
        whisper_model_size: Whisper ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large)
        temp_dir: ì„ì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
    
    Returns:
        í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ ë°œí™” êµ¬ê°„ ë¦¬ìŠ¤íŠ¸
    """
    import whisper
    
    logger.info(f"[STT] Whisper ëª¨ë¸ ë¡œë”© ì¤‘: {whisper_model_size}")
    model_whisper = whisper.load_model(whisper_model_size)
    
    # scipyë¥¼ ì‚¬ìš©í•˜ì—¬ wav íŒŒì¼ ë¡œë“œ
    sr, wav_np = wavfile.read(audio_path)
    
    # numpy dtypeì„ í™•ì¸í•˜ê³  ì •ê·œí™”
    if wav_np.dtype == np.int16:
        wav = torch.from_numpy(wav_np.astype(np.float32)) / 32768.0
    elif wav_np.dtype == np.int32:
        wav = torch.from_numpy(wav_np.astype(np.float32)) / 2147483648.0
    elif wav_np.dtype == np.float32:
        wav = torch.from_numpy(wav_np)
    else:
        wav = torch.from_numpy(wav_np.astype(np.float32))
    
    if wav.dim() == 1:
        wav = wav.unsqueeze(0)
    
    if temp_dir is None:
        temp_dir = os.path.dirname(audio_path)
    os.makedirs(temp_dir, exist_ok=True)
    
    logger.info(f"[STT] {len(speech_segments)}ê°œ ë°œí™” êµ¬ê°„ í…ìŠ¤íŠ¸ ì¸ì‹ ì¤‘...")
    
    for i, seg in enumerate(speech_segments):
        # í•´ë‹¹ ë°œí™” êµ¬ê°„ ì˜ë¼ë‚´ê¸°
        clip = wav[:, seg["start_sample"]:seg["end_sample"]]
        temp_path = os.path.join(temp_dir, f"temp_segment_{i}.wav")
        # scipyë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥
        clip_np = (clip.squeeze(0).numpy() * 32768).astype('int16')
        wavfile.write(temp_path, sr, clip_np)
        
        # Whisperë¡œ ì¸ì‹
        result = model_whisper.transcribe(temp_path, language=language)
        seg["text"] = result["text"].strip()
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_path):
            os.remove(temp_path)
        
        # ìƒ˜í”Œ ì •ë³´ëŠ” JSONì—ì„œ ì œì™¸
        del seg["start_sample"]
        del seg["end_sample"]
    
    logger.info("[STT] í…ìŠ¤íŠ¸ ì¸ì‹ ì™„ë£Œ")
    return speech_segments


def extract_frames(
    input_video: str,
    output_dir: str,
    fps: float = 2.0,
    output_format: str = "jpg",
    quality: int = 2
) -> List[Dict]:
    """
    ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        input_video: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        fps: ì¶”ì¶œí•  í”„ë ˆì„ ë ˆì´íŠ¸ (ê¸°ë³¸: 2fps)
        output_format: ì´ë¯¸ì§€ í¬ë§· (jpg, png)
        quality: JPEG í’ˆì§ˆ (1-31, ë‚®ì„ìˆ˜ë¡ ë†’ì€ í’ˆì§ˆ)
    
    Returns:
        í”„ë ˆì„ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{frame_number, timestamp, filename}, ...]
    """
    logger.info(f"[Frame] í”„ë ˆì„ ì¶”ì¶œ ì¤‘: {input_video} -> {output_dir} (fps={fps})")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    probe = ffmpeg.probe(input_video)
    video_stream = next(
        (stream for stream in probe['streams'] if stream['codec_type'] == 'video'),
        None
    )
    
    if video_stream is None:
        raise ValueError("ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    duration = float(probe['format']['duration'])
    logger.info(f"[Frame] ë¹„ë””ì˜¤ ê¸¸ì´: {duration:.2f}ì´ˆ")
    
    # í”„ë ˆì„ ì¶”ì¶œ (íŒŒì¼ëª…: frame_XXXX_T.TT.jpg í˜•ì‹)
    output_pattern = os.path.join(output_dir, f"frame_%04d.{output_format}")
    
    (
        ffmpeg
        .input(input_video)
        .filter('fps', fps=fps)
        .output(output_pattern, qscale=quality)
        .overwrite_output()
        .run(quiet=True)
    )
    
    # í”„ë ˆì„ ì •ë³´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    frame_files = sorted([f for f in os.listdir(output_dir) if f.startswith("frame_")])
    frame_info_list = []
    
    for i, filename in enumerate(frame_files):
        frame_number = i + 1
        timestamp = i / fps  # í”„ë ˆì„ ë²ˆí˜¸ì— ë”°ë¥¸ ì‹œê°„ ê³„ì‚°
        
        # íŒŒì¼ëª…ì„ ì‹œê°„ ì •ë³´ê°€ í¬í•¨ëœ ì´ë¦„ìœ¼ë¡œ ë³€ê²½
        new_filename = f"frame_{frame_number:04d}_{timestamp:.2f}s.{output_format}"
        old_path = os.path.join(output_dir, filename)
        new_path = os.path.join(output_dir, new_filename)
        os.rename(old_path, new_path)
        
        frame_info_list.append({
            "frame_number": frame_number,
            "timestamp": round(timestamp, 3),
            "filename": new_filename
        })
    
    logger.info(f"[Frame] {len(frame_info_list)}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ")
    return frame_info_list


def match_frames_to_segments(
    frame_info_list: List[Dict],
    speech_segments: List[Dict],
    silence_segments: List[Dict]
) -> Dict:
    """
    í”„ë ˆì„ê³¼ ëŒ€ì‚¬/ë¬´ìŒ êµ¬ê°„ì„ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­í•©ë‹ˆë‹¤.
    
    Args:
        frame_info_list: í”„ë ˆì„ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        speech_segments: ë°œí™” êµ¬ê°„ ë¦¬ìŠ¤íŠ¸
        silence_segments: ë¬´ìŒ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ë§¤ì¹­ ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("[Match] í”„ë ˆì„-êµ¬ê°„ ë§¤ì¹­ ì¤‘...")
    
    # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
    all_segments = sorted(
        speech_segments + silence_segments,
        key=lambda x: x["start_time"]
    )
    
    # ê° í”„ë ˆì„ì— í•´ë‹¹í•˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ ì°¾ê¸°
    for frame in frame_info_list:
        timestamp = frame["timestamp"]
        frame["segment_type"] = None
        frame["segment_id"] = None
        
        for seg in all_segments:
            if seg["start_time"] <= timestamp < seg["end_time"]:
                frame["segment_type"] = seg["type"]
                frame["segment_id"] = seg["id"]
                break
    
    # ë¬´ìŒ êµ¬ê°„ë³„ í”„ë ˆì„ ë§¤í•‘
    for silence in silence_segments:
        silence["frames"] = [
            f["filename"] for f in frame_info_list
            if silence["start_time"] <= f["timestamp"] < silence["end_time"]
        ]
        silence["frame_count"] = len(silence["frames"])
    
    logger.info("[Match] ë§¤ì¹­ ì™„ë£Œ")
    return {
        "frames": frame_info_list,
        "speech_segments": speech_segments,
        "silence_segments": silence_segments
    }


def process_video_for_gpt(
    input_video: str,
    output_dir: str,
    fps: float = 2.0,
    language: str = "en",
    whisper_model: str = "base",
    min_silence_duration: float = 0.5
) -> Dict:
    """
    GPT APIìš©ìœ¼ë¡œ ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        input_video: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        fps: í”„ë ˆì„ ì¶”ì¶œ ë ˆì´íŠ¸ (ê¸°ë³¸: 2fps)
        language: ì–¸ì–´ ì½”ë“œ (en, ko)
        whisper_model: Whisper ëª¨ë¸ í¬ê¸°
        min_silence_duration: ë¬´ìŒ ìµœì†Œ ì‹œê°„ (ì´ˆ)
    
    Returns:
        ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    video_name = Path(input_video).stem
    video_output_dir = os.path.join(output_dir, video_name)
    frames_dir = os.path.join(video_output_dir, "frames")
    audio_path = os.path.join(video_output_dir, "audio.wav")
    json_path = os.path.join(video_output_dir, f"{video_name}_data.json")
    
    os.makedirs(video_output_dir, exist_ok=True)
    os.makedirs(frames_dir, exist_ok=True)
    
    logger.info(f"[Pipeline] ì²˜ë¦¬ ì‹œì‘: {input_video}")
    logger.info(f"[Pipeline] ì¶œë ¥ ë””ë ‰í† ë¦¬: {video_output_dir}")
    
    # 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ
    extract_audio(input_video, audio_path)
    
    # 2. VADë¡œ ë°œí™”/ë¬´ìŒ êµ¬ê°„ íƒì§€
    vad_model, get_speech_timestamps_fn = load_vad_model()
    speech_segments, silence_segments, sr = detect_speech_segments(
        audio_path, vad_model, get_speech_timestamps_fn, min_silence_duration
    )
    
    # 3. Whisperë¡œ í…ìŠ¤íŠ¸ ì¸ì‹
    speech_segments = transcribe_speech_segments(
        audio_path, speech_segments, sr, language, whisper_model, video_output_dir
    )
    
    # 4. í”„ë ˆì„ ì¶”ì¶œ
    frame_info_list = extract_frames(input_video, frames_dir, fps)
    
    # 5. í”„ë ˆì„-êµ¬ê°„ ë§¤ì¹­
    matched_data = match_frames_to_segments(
        frame_info_list, speech_segments, silence_segments
    )
    
    # 6. ìµœì¢… JSON êµ¬ì¡° ìƒì„±
    result = {
        "video_info": {
            "source_file": os.path.basename(input_video),
            "fps_extracted": fps,
            "language": language,
            "total_frames": len(frame_info_list),
            "total_speech_segments": len(speech_segments),
            "total_silence_segments": len(silence_segments)
        },
        "speech_segments": speech_segments,
        "silence_segments": silence_segments,
        "frames": frame_info_list
    }
    
    # 7. JSON ì €ì¥
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    logger.info(f"[Pipeline] JSON ì €ì¥ ì™„ë£Œ: {json_path}")
    logger.info(f"[Pipeline] í”„ë ˆì„ ì €ì¥ ì™„ë£Œ: {frames_dir}")
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 50)
    print("GPT APIìš© ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    print("=" * 50)
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {video_output_dir}")
    print(f"ğŸ“„ JSON íŒŒì¼: {json_path}")
    print(f"ğŸ–¼ï¸  í”„ë ˆì„ í´ë”: {frames_dir}")
    print(f"\nğŸ“Š ìš”ì•½:")
    print(f"   - ì´ í”„ë ˆì„ ìˆ˜: {len(frame_info_list)}")
    print(f"   - ë°œí™” êµ¬ê°„: {len(speech_segments)}ê°œ")
    print(f"   - ë¬´ìŒ êµ¬ê°„: {len(silence_segments)}ê°œ (AD ì‚½ì… ê°€ëŠ¥)")
    print("=" * 50)
    
    return result


def main():
    parser = argparse.ArgumentParser(
        description="GPT APIìš© ì˜ìƒ ë°ì´í„° ì¶”ì¶œ (ëŒ€ì‚¬/ë¬´ìŒêµ¬ê°„ JSON + 2fps ì´ë¯¸ì§€)"
    )
    parser.add_argument("video_path", help="ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument(
        "--output_dir", "-o",
        default="./gpt_data",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ./gpt_data)"
    )
    parser.add_argument(
        "--fps", "-f",
        type=float,
        default=2.0,
        help="í”„ë ˆì„ ì¶”ì¶œ ë ˆì´íŠ¸ (ê¸°ë³¸: 2fps)"
    )
    parser.add_argument(
        "--language", "-l",
        default="en",
        choices=["en", "ko"],
        help="ì˜¤ë””ì˜¤ ì–¸ì–´ (ê¸°ë³¸: en)"
    )
    parser.add_argument(
        "--whisper_model", "-w",
        default="medium",
        choices=["tiny", "base", "small", "medium", "large"],
        help="Whisper ëª¨ë¸ í¬ê¸° (ê¸°ë³¸: medium)"
    )
    parser.add_argument(
        "--min_silence", "-s",
        type=float,
        default=0.5,
        help="ë¬´ìŒ êµ¬ê°„ ìµœì†Œ ì‹œê°„ (ì´ˆ, ê¸°ë³¸: 0.5)"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ë¹„ë””ì˜¤ ì²˜ë¦¬
    process_video_for_gpt(
        input_video=args.video_path,
        output_dir=args.output_dir,
        fps=args.fps,
        language=args.language,
        whisper_model=args.whisper_model,
        min_silence_duration=args.min_silence
    )


if __name__ == "__main__":
    main()

