"""
GPT APIë¥¼ í™œìš©í•œ Audio Description ìƒì„± ëª¨ë“ˆ (v2)

ê°œì„ ì‚¬í•­:
1. ê¸´ ë¬´ìŒêµ¬ê°„ ì¥ë©´ ë¶„í•  (ìµœì†Œ 8ì´ˆ ê¸°ì¤€)
2. Two-Pass ë°©ì‹: ì „ì²´ ë§¥ë½ íŒŒì•… í›„ ê°œë³„ AD ìƒì„±

ì…ë ¥:
- JSON íŒŒì¼: ëŒ€ì‚¬ ë° ë¬´ìŒ êµ¬ê°„ ì •ë³´
- ì´ë¯¸ì§€ í´ë”: 2fpsë¡œ ì¶”ì¶œëœ í”„ë ˆì„ ì´ë¯¸ì§€

ì¶œë ¥:
- AD JSON: ë¬´ìŒ êµ¬ê°„ë³„ Audio Description í…ìŠ¤íŠ¸
"""

import os
import json
import base64
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    env_paths = [
        Path(__file__).parent.parent.parent / ".env",
        Path(__file__).parent.parent / "server" / ".env",
    ]
    for env_path in env_paths:
        if env_path.exists():
            load_dotenv(env_path)
            break
except ImportError:
    pass

from openai import OpenAI

logger = logging.getLogger(__name__)

# GPT ëª¨ë¸ ì„¤ì •
GPT_MODEL = "gpt-4o"

# ============================================================
# í”„ë¡¬í”„íŠ¸ ì •ì˜
# ============================================================

# 1st Pass: ì „ì²´ ë§¥ë½ íŒŒì•… í”„ë¡¬í”„íŠ¸
CONTEXT_PROMPT_KO = """ë‹¹ì‹ ì€ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í‚¤í”„ë ˆì„ë“¤ê³¼ ì „ì²´ ëŒ€ë³¸ì„ ë¶„ì„í•˜ì—¬ ì˜ìƒì˜ ì „ì²´ì ì¸ ë§¥ë½ì„ íŒŒì•…í•´ì£¼ì„¸ìš”.

## ì „ì²´ ëŒ€ë³¸:
{full_transcript}

## ìš”ì²­:
ìœ„ í‚¤í”„ë ˆì„ë“¤({frame_count}ì¥)ê³¼ ëŒ€ë³¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ê°„ëµíˆ ì •ë¦¬í•´ì£¼ì„¸ìš”:
1. **ì•Œë ¤ì§„ ì½˜í…ì¸  í™•ì¸**: ì´ ì˜ìƒì´ ì•Œë ¤ì§„ ì˜í™”, ë“œë¼ë§ˆ, ì• ë‹ˆë©”ì´ì…˜ ë“±ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. 
   ì•Œë ¤ì§„ ì‘í’ˆì´ë¼ë©´ ì œëª©ê³¼ ì‹œì¦Œ/ì—í”¼ì†Œë“œ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”.
2. **ì¥ì†Œ/ë°°ê²½**: ì–´ë””ì„œ ì¼ì–´ë‚˜ëŠ” ì´ì•¼ê¸°ì¸ê°€?
3. **ë“±ì¥ì¸ë¬¼**: ì£¼ìš” ì¸ë¬¼ë“¤ì„ ì„±ë³„ê³¼ ë“±ì¥ ìˆœì„œë¡œ êµ¬ë¶„
   - ì´ë¦„ì´ë‚˜ ì™¸ëª¨ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  "ë‚¨ì1", "ë‚¨ì2", "ì—¬ì1", "ì—¬ì2" í˜•ì‹ìœ¼ë¡œ ì§€ì¹­
   - ë™ì¼ ì¸ë¬¼ì€ ì˜ìƒ ì „ì²´ì—ì„œ ì¼ê´€ëœ ë²ˆí˜¸ ìœ ì§€
   - ì˜ˆ: "ë‚¨ì1", "ë‚¨ì2", "ì—¬ì1" (ì´ë¦„ì´ë‚˜ ì™¸ëª¨ íŠ¹ì§• ì‚¬ìš© ê¸ˆì§€)
4. **ìƒí™© ìš”ì•½**: ì–´ë–¤ ìƒí™©ì´ ë²Œì–´ì§€ê³  ìˆëŠ”ê°€? (2-3ë¬¸ì¥)
5. **ë¶„ìœ„ê¸°**: ì „ì²´ì ì¸ ë¶„ìœ„ê¸° (ê¸´ì¥ê°, ìŠ¬í”” ë“±)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
```json
{{
  "known_content": {{
    "is_known": true/false,
    "title": "ì‘í’ˆ ì œëª© (ì•Œë ¤ì§„ ê²½ìš°)",
    "season_episode": "ì‹œì¦Œ/ì—í”¼ì†Œë“œ ì •ë³´ (í•´ë‹¹ë˜ëŠ” ê²½ìš°)",
    "description": "ì‘í’ˆì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª… (ì•Œë ¤ì§„ ê²½ìš°)"
  }},
  "location": "ì¥ì†Œ ì„¤ëª…",
  "characters": ["ì¸ë¬¼1 ì •ë³´", "ì¸ë¬¼2 ì •ë³´"],
  "situation": "ìƒí™© ìš”ì•½",
  "mood": "ë¶„ìœ„ê¸°"
}}
```
"""

CONTEXT_PROMPT_EN = """You are a video analysis expert.
Analyze the given keyframes and full transcript to understand the overall context.

## Full Transcript:
{full_transcript}

## Request:
Analyze the keyframes ({frame_count} frames) and transcript, then summarize:
1. **Known Content Check**: Identify if this is a known movie, TV series, animation, etc.
   If it's a known work, include the title and season/episode information.
2. **Location/Setting**: Where does this take place?
3. **Characters**: Identify main characters by gender and sequential number
   - Do NOT use names or physical descriptions; refer as "Man1", "Man2", "Woman1", "Woman2"
   - Maintain consistent numbering for the same character throughout the video
   - Example: "Man1", "Man2", "Woman1" (no names or appearance descriptions)
4. **Situation Summary**: What is happening? (2-3 sentences)
5. **Mood**: Overall mood (tension, sadness, etc.)

Respond in JSON format:
```json
{{
  "known_content": {{
    "is_known": true/false,
    "title": "Title of the work (if known)",
    "season_episode": "Season/Episode info (if applicable)",
    "description": "Brief description of the work (if known)"
  }},
  "location": "location description",
  "characters": ["character1 info", "character2 info"],
  "situation": "situation summary",
  "mood": "mood"
}}
```
"""

# 2nd Pass: AD ìƒì„± í”„ë¡¬í”„íŠ¸ (ë§¥ë½ í¬í•¨)
AD_PROMPT_KO = """ë‹¹ì‹ ì€ ì‹œê°ì¥ì• ì¸ì„ ìœ„í•œ í™”ë©´ í•´ì„¤(Audio Description) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì‘í’ˆ ì •ë³´:
{known_content_info}

## ì˜ìƒ ë§¥ë½ ì •ë³´ (ì´ë¯¸ ì‹œì²­ìì—ê²Œ ê³µìœ ë¨):
- **ì¥ì†Œ**: {location}
- **ë“±ì¥ì¸ë¬¼**: {characters}
- **ìƒí™©**: {situation}
- **ë¶„ìœ„ê¸°**: {mood}

## ì§ì „ í™”ë©´ í•´ì„¤:
{prev_ad}

## ì´ì „ ëŒ€ì‚¬:
{prev_context}

## ë‹¤ìŒ ëŒ€ì‚¬:
{next_context}

## ê·œì¹™:
1. **ê°„ê²°í•˜ê²Œ**: {duration}ì´ˆ ë‚´ì— ì½ì„ ìˆ˜ ìˆë„ë¡ ì§§ê³  í•µì‹¬ì ì¸ ë¬¸ì¥
2. **ì‹œê°ì  ì •ë³´ë§Œ**: ì†Œë¦¬ë¡œ ì•Œ ìˆ˜ ì—†ëŠ”, ëˆˆìœ¼ë¡œë§Œ ë³¼ ìˆ˜ ìˆëŠ” ì •ë³´
3. **í˜„ì¬ ì‹œì œ**: "~í•˜ê³  ìˆë‹¤", "~í•œë‹¤" í˜•íƒœ
4. **ë“±ì¥ì¸ë¬¼ ì§€ì¹­ (ë§¤ìš° ì¤‘ìš”)**: 
   - ë“±ì¥ì¸ë¬¼ì˜ ì´ë¦„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
   - ì™¸ëª¨, ì˜·ì°¨ë¦¼, ë‚˜ì´ ë“±ì„ ë¬˜ì‚¬í•˜ì§€ ë§ˆì„¸ìš”
   - ì„±ë³„ê³¼ ë“±ì¥ ìˆœì„œë¡œë§Œ ì§€ì¹­: "ë‚¨ì1", "ë‚¨ì2", "ì—¬ì1", "ì—¬ì2" ë“±
   - ë™ì¼ ì¸ë¬¼ì€ ì˜ìƒ ì „ì²´ì—ì„œ ì¼ê´€ëœ ë²ˆí˜¸ ìœ ì§€
   - ì˜ˆ: "ì² ìˆ˜ê°€ ë¬¸ì„ ì—°ë‹¤" (X) â†’ "ë‚¨ì1ì´ ë¬¸ì„ ì—°ë‹¤" (O)
5. **ì‚¬ì „ ì§€ì‹ í™œìš©**: ì•Œë ¤ì§„ ì‘í’ˆì´ë¼ë„ ìºë¦­í„° ì´ë¦„ ëŒ€ì‹  "ë‚¨ì1", "ì—¬ì1" í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
6. **ì¤‘ë³µ í”¼í•˜ê¸° (ë§¤ìš° ì¤‘ìš”)**:
   - ì˜ìƒ ë§¥ë½ ì •ë³´(ì¥ì†Œ, ë¶„ìœ„ê¸°)ëŠ” ì´ë¯¸ ì‹œì²­ìê°€ ì•Œê³  ìˆìœ¼ë¯€ë¡œ ë§¤ë²ˆ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
   - ì§ì „ í™”ë©´ í•´ì„¤ì—ì„œ ì–¸ê¸‰í•œ ë‚´ìš©(ì¥ì†Œ, ë°°ê²½, ë‚ ì”¨, í™˜ê²½)ì€ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
   - ìƒˆë¡œìš´ í–‰ë™, ë³€í™”, ì›€ì§ì„ì— ì§‘ì¤‘í•˜ì„¸ìš”
   - ì¥ë©´ì´ í¬ê²Œ ë°”ë€Œì§€ ì•Šì•˜ë‹¤ë©´ ë°°ê²½ ë¬˜ì‚¬ë¥¼ ìƒëµí•˜ê³  ì¸ë¬¼ì˜ ë™ì‘ë§Œ ì„¤ëª…í•˜ì„¸ìš”

## ìš”ì²­:
ìœ„ ì´ë¯¸ì§€ë“¤({frame_count}ì¥, ì‹œê°„: {start_time}s ~ {end_time}s)ì—ì„œ 
{duration}ì´ˆ ì•ˆì— ì½ì„ ìˆ˜ ìˆëŠ” í™”ë©´ í•´ì„¤ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‘ë‹µì€ í™”ë©´ í•´ì„¤ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

AD_PROMPT_EN = """You are an Audio Description expert for visually impaired viewers.

## Content Information:
{known_content_info}

## Video Context (already known to the viewer):
- **Location**: {location}
- **Characters**: {characters}
- **Situation**: {situation}
- **Mood**: {mood}

## Previous Audio Description:
{prev_ad}

## Previous dialogue:
{prev_context}

## Next dialogue:
{next_context}

## Rules:
1. **Be concise**: Short sentences readable within {duration} seconds
2. **Visual only**: Information not available through sound
3. **Present tense**: Use present tense
4. **Character reference (CRITICAL)**: 
   - NEVER use character names
   - Do NOT describe physical appearance, clothing, or age
   - Refer to characters ONLY by gender and sequential number: "Man1", "Man2", "Woman1", "Woman2"
   - Maintain consistent numbering for the same character throughout the video
   - Example: "John opens the door" (X) â†’ "Man1 opens the door" (O)
5. **Leverage prior knowledge**: Even for known content, use "Man1", "Woman1" format instead of character names.
6. **AVOID REPETITION (CRITICAL)**:
   - The video context (location, mood) is already known to the viewer - do NOT repeat it every time
   - Do NOT repeat information from the previous Audio Description (location, setting, weather, environment)
   - Focus on NEW actions, changes, and movements
   - If the scene hasn't changed significantly, skip background descriptions and focus on character actions only

## Request:
Write an Audio Description for the images ({frame_count} frames, time: {start_time}s ~ {end_time}s)
that can be read within {duration} seconds.

Output only the Audio Description text.
"""


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================

def encode_image_to_base64(image_path: str) -> str:
    """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def get_image_media_type(image_path: str) -> str:
    """ì´ë¯¸ì§€ì˜ MIME íƒ€ì…ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    ext = Path(image_path).suffix.lower()
    media_types = {
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".png": "image/png",
        ".gif": "image/gif",
        ".webp": "image/webp"
    }
    return media_types.get(ext, "image/jpeg")


def split_long_silence(silence_segment: Dict, frame_info_list: List[Dict], min_split_duration: float = 8.0) -> List[Dict]:
    """
    ê¸´ ë¬´ìŒêµ¬ê°„ì„ ì¥ë©´ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    
    ê·œì¹™:
    - 16ì´ˆ ë¯¸ë§Œ: ë¶„í•  ì—†ìŒ
    - 16ì´ˆ ì´ìƒ: 8ì´ˆ ë‹¨ìœ„ë¡œ ë¶„í• , ë§ˆì§€ë§‰ì€ ë‚˜ë¨¸ì§€
    
    ì˜ˆì‹œ:
    - 15ì´ˆ â†’ [15ì´ˆ]
    - 16ì´ˆ â†’ [8ì´ˆ, 8ì´ˆ]
    - 23ì´ˆ â†’ [8ì´ˆ, 15ì´ˆ]
    - 30ì´ˆ â†’ [8ì´ˆ, 8ì´ˆ, 14ì´ˆ]
    
    Args:
        silence_segment: ë¬´ìŒ êµ¬ê°„ ì •ë³´
        frame_info_list: ì „ì²´ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
        min_split_duration: ìµœì†Œ ë¶„í•  ë‹¨ìœ„ (ê¸°ë³¸: 8ì´ˆ)
    
    Returns:
        ë¶„í• ëœ ë¬´ìŒ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸
    """
    duration = silence_segment["duration"]
    start_time = silence_segment["start_time"]
    end_time = silence_segment["end_time"]
    original_id = silence_segment["id"]
    
    # 16ì´ˆ ë¯¸ë§Œì´ë©´ ë¶„í• í•˜ì§€ ì•ŠìŒ
    if duration < min_split_duration * 2:
        return [silence_segment]
    
    # ë¶„í•  ê°œìˆ˜ ê³„ì‚°: durationì„ 8ì´ˆë¡œ ë‚˜ëˆˆ ëª« (ìµœì†Œ 2ê°œ)
    num_splits = max(2, int(duration // min_split_duration))
    
    # ë§ˆì§€ë§‰ êµ¬ê°„ì´ 8ì´ˆ ë¯¸ë§Œì´ ë˜ì§€ ì•Šë„ë¡ ì¡°ì •
    # ì˜ˆ: 23ì´ˆ â†’ 8ì´ˆ + 15ì´ˆ (2ê°œ), 30ì´ˆ â†’ 8ì´ˆ + 8ì´ˆ + 14ì´ˆ (3ê°œ)
    if duration - (num_splits - 1) * min_split_duration < min_split_duration:
        num_splits -= 1
    
    split_segments = []
    current_start = start_time
    
    for i in range(num_splits):
        if i == num_splits - 1:
            # ë§ˆì§€ë§‰ êµ¬ê°„: ë‚˜ë¨¸ì§€ ì „ë¶€
            split_end = end_time
        else:
            # 8ì´ˆ êµ¬ê°„
            split_end = current_start + min_split_duration
        
        split_duration = split_end - current_start
        
        # í•´ë‹¹ êµ¬ê°„ì˜ í”„ë ˆì„ ì°¾ê¸°
        split_frames = [
            f["filename"] for f in frame_info_list
            if current_start <= f["timestamp"] < split_end
        ]
        
        split_segments.append({
            "id": f"{original_id}_{i+1}",
            "original_id": original_id,
            "start_time": round(current_start, 3),
            "end_time": round(split_end, 3),
            "duration": round(split_duration, 3),
            "type": "silence",
            "frames": split_frames,
            "frame_count": len(split_frames),
            "is_split": True,
            "split_index": i + 1,
            "total_splits": num_splits
        })
        
        current_start = split_end
    
    logger.info(f"[Split] ë¬´ìŒêµ¬ê°„ {original_id} ({duration:.1f}ì´ˆ) â†’ {num_splits}ê°œë¡œ ë¶„í• ")
    for seg in split_segments:
        logger.info(f"  - {seg['id']}: {seg['start_time']:.1f}s ~ {seg['end_time']:.1f}s ({seg['duration']:.1f}ì´ˆ, {seg['frame_count']}í”„ë ˆì„)")
    
    return split_segments


def get_context_text(segments: List[Dict], current_start_time: float, direction: str = "prev") -> str:
    """í˜„ì¬ ë¬´ìŒ êµ¬ê°„ ì „í›„ì˜ ëŒ€ì‚¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    context_parts = []
    
    speech_segments = [s for s in segments if s.get("type") == "speech" and s.get("text")]
    
    if direction == "prev":
        # í˜„ì¬ ì‹œì‘ ì‹œê°„ë³´ë‹¤ ì´ì „ì— ëë‚˜ëŠ” ëŒ€ì‚¬ë“¤
        prev_speeches = [s for s in speech_segments if s["end_time"] <= current_start_time]
        prev_speeches = sorted(prev_speeches, key=lambda x: x["end_time"], reverse=True)[:2]
        for s in reversed(prev_speeches):
            context_parts.append(f"[{s['start_time']:.1f}s] {s['text']}")
    else:
        # í˜„ì¬ ë ì‹œê°„ë³´ë‹¤ ì´í›„ì— ì‹œì‘í•˜ëŠ” ëŒ€ì‚¬ë“¤ (current_start_timeì€ ì‹¤ì œë¡œ end_timeì„ ì „ë‹¬)
        next_speeches = [s for s in speech_segments if s["start_time"] >= current_start_time]
        next_speeches = sorted(next_speeches, key=lambda x: x["start_time"])[:2]
        for s in next_speeches:
            context_parts.append(f"[{s['start_time']:.1f}s] {s['text']}")
    
    return "\n".join(context_parts) if context_parts else "(ì—†ìŒ / None)"


# ============================================================
# 1st Pass: ì „ì²´ ë§¥ë½ íŒŒì•…
# ============================================================

def analyze_video_context(
    client: OpenAI,
    frames_dir: str,
    frame_info_list: List[Dict],
    speech_segments: List[Dict],
    language: str = "ko",
    num_keyframes: int = 15
) -> Dict:
    """
    ì „ì²´ ì˜ìƒì˜ ë§¥ë½ì„ íŒŒì•…í•©ë‹ˆë‹¤ (1st Pass).
    
    Args:
        client: OpenAI í´ë¼ì´ì–¸íŠ¸
        frames_dir: í”„ë ˆì„ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
        frame_info_list: ì „ì²´ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
        speech_segments: ë°œí™” êµ¬ê°„ ë¦¬ìŠ¤íŠ¸
        language: ì–¸ì–´ (ko, en)
        num_keyframes: ì‚¬ìš©í•  í‚¤í”„ë ˆì„ ìˆ˜
    
    Returns:
        ë§¥ë½ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("[Context] 1st Pass: ì „ì²´ ë§¥ë½ íŒŒì•… ì¤‘...")
    
    # ì „ì²´ ëŒ€ë³¸ êµ¬ì„±
    full_transcript = "\n".join([
        f"[{s['start_time']:.1f}s] {s.get('text', '')}"
        for s in sorted(speech_segments, key=lambda x: x['start_time'])
        if s.get('text')
    ])
    
    # í‚¤í”„ë ˆì„ ê· ë“± ìƒ˜í”Œë§
    if len(frame_info_list) > num_keyframes:
        step = len(frame_info_list) / num_keyframes
        selected_indices = [int(i * step) for i in range(num_keyframes)]
        keyframes = [frame_info_list[i] for i in selected_indices]
    else:
        keyframes = frame_info_list
    
    # í”„ë¡¬í”„íŠ¸ ì„ íƒ
    prompt_template = CONTEXT_PROMPT_KO if language == "ko" else CONTEXT_PROMPT_EN
    prompt = prompt_template.format(
        full_transcript=full_transcript[:3000],  # í† í° ì œí•œ
        frame_count=len(keyframes)
    )
    
    # ë©”ì‹œì§€ êµ¬ì„±
    content = [{"type": "text", "text": prompt}]
    
    for frame in keyframes:
        image_path = os.path.join(frames_dir, frame["filename"])
        if os.path.exists(image_path):
            base64_image = encode_image_to_base64(image_path)
            media_type = get_image_media_type(image_path)
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:{media_type};base64,{base64_image}",
                    "detail": "low"
                }
            })
    
    # GPT API í˜¸ì¶œ
    try:
        response = client.chat.completions.create(
            model=GPT_MODEL,
            messages=[{"role": "user", "content": content}],
            max_tokens=500,
            temperature=0.3
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # JSON íŒŒì‹± ì‹œë„
        import re
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            context = json.loads(json_match.group())
        else:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            context = {
                "known_content": {"is_known": False},
                "location": "ì•Œ ìˆ˜ ì—†ìŒ",
                "characters": ["ì¸ë¬¼1", "ì¸ë¬¼2"],
                "situation": response_text[:200],
                "mood": "ì•Œ ìˆ˜ ì—†ìŒ"
            }
        
        # known_content ê¸°ë³¸ê°’ ë³´ì¥
        if "known_content" not in context:
            context["known_content"] = {"is_known": False}
        
        known = context.get("known_content", {})
        logger.info(f"[Context] ë§¥ë½ íŒŒì•… ì™„ë£Œ:")
        if known.get("is_known"):
            logger.info(f"  - ğŸ¬ ì•Œë ¤ì§„ ì‘í’ˆ: {known.get('title', 'N/A')} ({known.get('season_episode', 'N/A')})")
        logger.info(f"  - ì¥ì†Œ: {context.get('location', 'N/A')}")
        logger.info(f"  - ì¸ë¬¼: {context.get('characters', [])}")
        logger.info(f"  - ìƒí™©: {context.get('situation', 'N/A')[:50]}...")
        
        return context
        
    except Exception as e:
        logger.error(f"[Context] ë§¥ë½ íŒŒì•… ì‹¤íŒ¨: {e}")
        return {
            "known_content": {"is_known": False},
            "location": "ì•Œ ìˆ˜ ì—†ìŒ",
            "characters": ["ì¸ë¬¼1", "ì¸ë¬¼2"],
            "situation": "ìƒí™© íŒŒì•… ì‹¤íŒ¨",
            "mood": "ì•Œ ìˆ˜ ì—†ìŒ"
        }


# ============================================================
# 2nd Pass: ê°œë³„ AD ìƒì„±
# ============================================================

def generate_ad_for_segment(
    client: OpenAI,
    segment: Dict,
    frames_dir: str,
    all_segments: List[Dict],
    context: Dict,
    prev_ad: str = "",
    language: str = "ko",
    max_frames: int = 10
) -> str:
    """
    ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ADë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (2nd Pass).
    
    Args:
        prev_ad: ì§ì „ ì„¸ê·¸ë¨¼íŠ¸ì˜ AD í…ìŠ¤íŠ¸ (ì¤‘ë³µ ë°©ì§€ìš©)
    """
    frame_files = segment.get("frames", [])
    
    if not frame_files:
        logger.warning(f"[AD] ì„¸ê·¸ë¨¼íŠ¸ {segment['id']}ì— í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return ""
    
    # í”„ë ˆì„ ìƒ˜í”Œë§
    if len(frame_files) > max_frames:
        step = len(frame_files) / max_frames
        selected_indices = [int(i * step) for i in range(max_frames)]
        frame_files = [frame_files[i] for i in selected_indices]
    
    # ì»¨í…ìŠ¤íŠ¸
    prev_context = get_context_text(all_segments, segment["start_time"], "prev")
    next_context = get_context_text(all_segments, segment["end_time"], "next")
    
    # ì•Œë ¤ì§„ ì½˜í…ì¸  ì •ë³´ êµ¬ì„±
    known_content = context.get("known_content", {})
    if known_content.get("is_known"):
        known_content_info = f"""ì´ ì˜ìƒì€ ì•Œë ¤ì§„ ì‘í’ˆì…ë‹ˆë‹¤.
- ì œëª©: {known_content.get('title', 'N/A')}
- ì‹œì¦Œ/ì—í”¼ì†Œë“œ: {known_content.get('season_episode', 'N/A')}
- ì„¤ëª…: {known_content.get('description', 'N/A')}
â†’ ë‹¹ì‹ ì´ ì´ ì‘í’ˆì— ëŒ€í•´ ì•Œê³  ìˆëŠ” ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë” ì •í™•í•œ í™”ë©´ í•´ì„¤ì„ ì‘ì„±í•˜ì„¸ìš”."""
    else:
        known_content_info = "ì´ ì˜ìƒì€ ì•Œë ¤ì§€ì§€ ì•Šì€ ì½˜í…ì¸ ì…ë‹ˆë‹¤. í™”ë©´ì— ë³´ì´ëŠ” ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì„¤í•˜ì„¸ìš”."
    
    # ì§ì „ AD ì •ë³´ êµ¬ì„±
    if prev_ad:
        prev_ad_text = f'"{prev_ad}"\nâ†’ ìœ„ ë‚´ìš©ì€ ì´ë¯¸ ì–¸ê¸‰í–ˆìœ¼ë¯€ë¡œ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”. ìƒˆë¡œìš´ í–‰ë™/ë³€í™”ì— ì§‘ì¤‘í•˜ì„¸ìš”.'
    else:
        prev_ad_text = "(ì²« ë²ˆì§¸ í™”ë©´ í•´ì„¤ì…ë‹ˆë‹¤. ê°„ëµí•˜ê²Œ ì¥ë©´ì„ ì„¤ì •í•˜ì„¸ìš”.)"
    
    # í”„ë¡¬í”„íŠ¸
    prompt_template = AD_PROMPT_KO if language == "ko" else AD_PROMPT_EN
    prompt = prompt_template.format(
        known_content_info=known_content_info,
        location=context.get("location", "ì•Œ ìˆ˜ ì—†ìŒ"),
        characters=", ".join(context.get("characters", [])),
        situation=context.get("situation", ""),
        mood=context.get("mood", ""),
        prev_ad=prev_ad_text,
        prev_context=prev_context,
        next_context=next_context,
        duration=segment["duration"],
        frame_count=len(frame_files),
        start_time=segment["start_time"],
        end_time=segment["end_time"]
    )
    
    # ë©”ì‹œì§€ êµ¬ì„±
    content = [{"type": "text", "text": prompt}]
    
    for frame_file in frame_files:
        image_path = os.path.join(frames_dir, frame_file)
        if os.path.exists(image_path):
            base64_image = encode_image_to_base64(image_path)
            media_type = get_image_media_type(image_path)
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:{media_type};base64,{base64_image}",
                    "detail": "low"
                }
            })
    
    # GPT API í˜¸ì¶œ
    try:
        response = client.chat.completions.create(
            model=GPT_MODEL,
            messages=[{"role": "user", "content": content}],
            max_tokens=200,
            temperature=0.3
        )
        
        ad_text = response.choices[0].message.content.strip()
        logger.info(f"[AD] ì„¸ê·¸ë¨¼íŠ¸ {segment['id']} ({segment['start_time']:.1f}s~{segment['end_time']:.1f}s): {ad_text[:50]}...")
        return ad_text
        
    except Exception as e:
        logger.error(f"[AD] GPT API ì˜¤ë¥˜: {e}")
        return ""


# ============================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================

def generate_ad_from_extracted_data(
    data_dir: str,
    api_key: str = None,
    language: str = "ko",
    min_duration: float = 2.5,
    max_frames_per_segment: int = 10,
    min_split_duration: float = 8.0
) -> Dict:
    """
    ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ADë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        api_key: OpenAI API í‚¤
        language: ì–¸ì–´ (ko, en)
        min_duration: AD ìƒì„± ìµœì†Œ ë¬´ìŒ êµ¬ê°„ ì‹œê°„
        max_frames_per_segment: ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ìµœëŒ€ í”„ë ˆì„ ìˆ˜
        min_split_duration: ì¥ë©´ ë¶„í•  ìµœì†Œ ë‹¨ìœ„ (ê¸°ë³¸: 8ì´ˆ)
    """
    # API í‚¤ ì„¤ì •
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY")
    
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    client = OpenAI(api_key=api_key)
    
    # ë°ì´í„° ë¡œë“œ
    data_dir = Path(data_dir)
    json_files = list(data_dir.glob("*_data.json"))
    
    if not json_files:
        raise FileNotFoundError(f"ë°ì´í„° JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
    
    json_path = json_files[0]
    frames_dir = data_dir / "frames"
    
    logger.info(f"[AD] ë°ì´í„° ë¡œë”©: {json_path}")
    
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    speech_segments = data.get("speech_segments", [])
    silence_segments = data.get("silence_segments", [])
    frame_info_list = data.get("frames", [])
    
    # ============================================================
    # 1st Pass: ì „ì²´ ë§¥ë½ íŒŒì•…
    # ============================================================
    context = analyze_video_context(
        client=client,
        frames_dir=str(frames_dir),
        frame_info_list=frame_info_list,
        speech_segments=speech_segments,
        language=language
    )
    
    # ============================================================
    # ê¸´ ë¬´ìŒêµ¬ê°„ ë¶„í• 
    # ============================================================
    logger.info("[Split] ê¸´ ë¬´ìŒêµ¬ê°„ ë¶„í•  ì²˜ë¦¬ ì¤‘...")
    
    all_ad_segments = []
    for silence in silence_segments:
        if silence["duration"] >= min_duration:
            # ê¸´ êµ¬ê°„ì€ ë¶„í• 
            split_segments = split_long_silence(
                silence_segment=silence,
                frame_info_list=frame_info_list,
                min_split_duration=min_split_duration
            )
            all_ad_segments.extend(split_segments)
    
    logger.info(f"[AD] ì´ AD ìƒì„± ëŒ€ìƒ: {len(all_ad_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
    
    # ì‹œê°„ìˆœ ì •ë ¬
    all_ad_segments = sorted(all_ad_segments, key=lambda x: x["start_time"])
    all_segments = sorted(speech_segments + silence_segments, key=lambda x: x["start_time"])
    
    # ============================================================
    # 2nd Pass: ê°œë³„ AD ìƒì„±
    # ============================================================
    logger.info("[AD] 2nd Pass: ê°œë³„ AD ìƒì„± ì¤‘...")
    
    audio_descriptions = []
    prev_ad = ""  # ì´ì „ AD ì¶”ì  (ì¤‘ë³µ ë°©ì§€ìš©)
    
    for segment in all_ad_segments:
        ad_text = generate_ad_for_segment(
            client=client,
            segment=segment,
            frames_dir=str(frames_dir),
            all_segments=all_segments,
            context=context,
            prev_ad=prev_ad,
            language=language,
            max_frames=max_frames_per_segment
        )
        
        if ad_text:
            prev_ad = ad_text  # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìœ„í•´ ì €ì¥
            audio_descriptions.append({
                "id": segment["id"],
                "original_id": segment.get("original_id", segment["id"]),
                "start_time": segment["start_time"],
                "end_time": segment["end_time"],
                "duration_sec": segment["duration"],
                "description": ad_text,
                "is_split": segment.get("is_split", False),
                "split_info": f"{segment.get('split_index', 1)}/{segment.get('total_splits', 1)}" if segment.get("is_split") else None
            })
    
    # ê²°ê³¼ êµ¬ì„±
    result = {
        "video_info": data.get("video_info", {}),
        "video_context": context,
        "full_transcript": [
            {
                "time": f"{s['start_time']:.1f}",
                "speaker": "Speaker",
                "text": s.get("text", "")
            }
            for s in sorted(speech_segments, key=lambda x: x["start_time"])
        ],
        "audio_descriptions": audio_descriptions
    }
    
    # ê²°ê³¼ ì €ì¥
    output_path = data_dir / f"{data_dir.name}.ad.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    logger.info(f"[AD] ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 60)
    print("GPT AD ìƒì„± ì™„ë£Œ (v2 - ì¥ë©´ë¶„í•  + Two-Pass)")
    print("=" * 60)
    print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {output_path}")
    print(f"ğŸ“Š ìƒì„±ëœ AD: {len(audio_descriptions)}ê°œ")
    print(f"ğŸ“ ë§¥ë½ ì •ë³´:")
    print(f"   - ì¥ì†Œ: {context.get('location', 'N/A')}")
    print(f"   - ì¸ë¬¼: {', '.join(context.get('characters', []))}")
    print(f"   - ë¶„ìœ„ê¸°: {context.get('mood', 'N/A')}")
    
    # ë¶„í• ëœ êµ¬ê°„ ì •ë³´
    split_count = sum(1 for ad in audio_descriptions if ad.get("is_split"))
    if split_count > 0:
        print(f"âœ‚ï¸  ë¶„í• ëœ êµ¬ê°„: {split_count}ê°œ")
    
    print("=" * 60)
    
    return result


def main():
    parser = argparse.ArgumentParser(
        description="GPT APIë¥¼ í™œìš©í•œ Audio Description ìƒì„± (v2)"
    )
    parser.add_argument(
        "data_dir",
        help="extract_for_gpt.pyë¡œ ìƒì„±ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ"
    )
    parser.add_argument(
        "--api_key", "-k",
        help="OpenAI API í‚¤ (ë˜ëŠ” OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--language", "-l",
        default="ko",
        choices=["ko", "en"],
        help="AD ìƒì„± ì–¸ì–´ (ê¸°ë³¸: ko)"
    )
    parser.add_argument(
        "--min_duration", "-d",
        type=float,
        default=2.5,
        help="AD ìƒì„± ìµœì†Œ ë¬´ìŒ êµ¬ê°„ ì‹œê°„ (ì´ˆ, ê¸°ë³¸: 2.5)"
    )
    parser.add_argument(
        "--max_frames", "-m",
        type=int,
        default=10,
        help="ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ìµœëŒ€ í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸: 10)"
    )
    parser.add_argument(
        "--min_split", "-s",
        type=float,
        default=8.0,
        help="ì¥ë©´ ë¶„í•  ìµœì†Œ ë‹¨ìœ„ (ì´ˆ, ê¸°ë³¸: 8)"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    generate_ad_from_extracted_data(
        data_dir=args.data_dir,
        api_key=args.api_key,
        language=args.language,
        min_duration=args.min_duration,
        max_frames_per_segment=args.max_frames,
        min_split_duration=args.min_split
    )


if __name__ == "__main__":
    main()
